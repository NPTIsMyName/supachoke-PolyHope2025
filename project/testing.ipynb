{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11092537,"sourceType":"datasetVersion","datasetId":6914708},{"sourceId":11196439,"sourceType":"datasetVersion","datasetId":6990208},{"sourceId":11256147,"sourceType":"datasetVersion","datasetId":7034614},{"sourceId":305660,"sourceType":"modelInstanceVersion","modelInstanceId":260785,"modelId":281942},{"sourceId":318363,"sourceType":"modelInstanceVersion","modelInstanceId":268651,"modelId":289670},{"sourceId":318493,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":268752,"modelId":289776}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:51:08.173812Z","iopub.execute_input":"2025-04-02T19:51:08.174150Z","iopub.status.idle":"2025-04-02T19:51:08.544169Z","shell.execute_reply.started":"2025-04-02T19:51:08.174124Z","shell.execute_reply":"2025-04-02T19:51:08.543417Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hope/transformers/default/1/spm.model\n/kaggle/input/hope/transformers/default/1/config.json\n/kaggle/input/hope/transformers/default/1/tokenizer.json\n/kaggle/input/hope/transformers/default/1/tokenizer_config.json\n/kaggle/input/hope/transformers/default/1/model.safetensors\n/kaggle/input/hope/transformers/default/1/special_tokens_map.json\n/kaggle/input/hope/transformers/default/1/added_tokens.json\n/kaggle/input/en-test-cleaned/cleaned_en_test.csv\n/kaggle/input/test-phase/en_test_without_labels.csv\n/kaggle/input/test-phase/es_test_without_labels.csv\n/kaggle/input/0.72-multiclass/transformers/default/1/config.json\n/kaggle/input/0.72-multiclass/transformers/default/1/tokenizer.json\n/kaggle/input/0.72-multiclass/transformers/default/1/tokenizer_config.json\n/kaggle/input/0.72-multiclass/transformers/default/1/model.safetensors\n/kaggle/input/0.72-multiclass/transformers/default/1/special_tokens_map.json\n/kaggle/input/0.72-multiclass/transformers/default/1/vocab.txt\n/kaggle/input/bge-binary/transformers/default/1/config.json\n/kaggle/input/bge-binary/transformers/default/1/tokenizer.json\n/kaggle/input/bge-binary/transformers/default/1/tokenizer_config.json\n/kaggle/input/bge-binary/transformers/default/1/model.safetensors\n/kaggle/input/bge-binary/transformers/default/1/special_tokens_map.json\n/kaggle/input/bge-binary/transformers/default/1/vocab.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\n# model path for tokenizer\nmodel_name = r\"model folder path\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nclass TestDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.data = dataframe\n        \n        self.text = dataframe[\"text\"]  \n        self.targets = dataframe[\"label\"] if \"label\" in dataframe.columns else None \n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        text = str(self.text[idx]).strip()\n\n        # Tokenization\n        inputs = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            truncation=False,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        item = {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n        }\n\n        if self.targets is not None:\n            item[\"label\"] = torch.tensor(self.targets[idx], dtype=torch.long)\n\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:51:08.545599Z","iopub.execute_input":"2025-04-02T19:51:08.546074Z","iopub.status.idle":"2025-04-02T19:51:16.166235Z","shell.execute_reply.started":"2025-04-02T19:51:08.546043Z","shell.execute_reply":"2025-04-02T19:51:16.165251Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\n# model folder path \nmodel_path = \"model folder path\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 2)\ndevice = torch.device('cuda')\n\nbatch_size = 32\n\n# test file path\n\ntest_file = r'test data path'\ntest_df = pd.read_csv(test_file)\ntest_dataset = TestDataset(test_df, tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\ntest_dataset = TestDataset(test_df, tokenizer)  #Dùng trực tiếp\n\nmodel.to(device)\nmodel.eval()\n\n# multiclass\nlabel_map = {\n    0: \"Generalized Hope\",\n    1: \"Not Hope\",\n    2: \"Realistic Hope\",\n    3: \"Sarcasm\",\n    4: \"Unrealistic Hope\"\n}\n\n# binary\nlabel_map_ = {\n    0: \"Not Hope\",\n    1: \"Hope\"\n}\n\ntest_predictions = []\ntest_labels = []\n\nwith torch.no_grad():\n    for i, batch in enumerate(test_loader, 1):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        \n        batch_size = input_ids.size(0)\n        test_labels.extend([f\"text{j}\" for j in range(i, i + batch_size)])\n        \n        test_predictions.extend(predicted.cpu().numpy())\n\ny_pred = [label_map_[label] for label in test_predictions]\n\ndf = pd.DataFrame({\"Text\": test_labels, \"Tag\": y_pred})  \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:54:27.919644Z","iopub.execute_input":"2025-04-02T19:54:27.919976Z","iopub.status.idle":"2025-04-02T19:58:36.822683Z","shell.execute_reply.started":"2025-04-02T19:54:27.919953Z","shell.execute_reply":"2025-04-02T19:58:36.821943Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"save_path = \"save predicted  path\"\ndf.to_csv(save_path, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:58:36.823911Z","iopub.execute_input":"2025-04-02T19:58:36.824215Z","iopub.status.idle":"2025-04-02T19:58:36.836112Z","shell.execute_reply.started":"2025-04-02T19:58:36.824188Z","shell.execute_reply":"2025-04-02T19:58:36.835444Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import zipfile\n\nsave_path = \"save predicted path\"\nzip_path = \"zip predicted file path\"\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write(save_path, arcname=\"prediction file name\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:58:36.837377Z","iopub.execute_input":"2025-04-02T19:58:36.837688Z","iopub.status.idle":"2025-04-02T19:58:36.843777Z","shell.execute_reply.started":"2025-04-02T19:58:36.837657Z","shell.execute_reply":"2025-04-02T19:58:36.843061Z"}},"outputs":[],"execution_count":6}]}